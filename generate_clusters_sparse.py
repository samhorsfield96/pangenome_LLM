
import argparse
from collections import OrderedDict
import pandas as pd
import pickle
import numpy as np
import scipy as sp
from scipy.sparse import coo_matrix, csr_matrix
import os

from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score
from sklearn.preprocessing import normalize
import scanpy as sc

def get_options():
    description = "Generates Leiden clusters of Sketchlib sparse matrices"
    parser = argparse.ArgumentParser(description=description,
                                        prog='python generate_clusters_sparse.py')
    IO = parser.add_argument_group('Input/options.out')
    IO.add_argument('--distances',
                    required=True,
                    help='Distances .npz file generated by ppsketchlib')
    IO.add_argument('--samples',
                    required=True,
                    help='Samples .pkl file generated by ppsketchlib')
    IO.add_argument('--labels',
                    default=None,
                    help='PopPUNK csv file describing genome names in first column in same order as in embeddings file. Can have second column with assigned clusters.')
    IO.add_argument('--outpref',
                default="output",
                help='Output prefix. Default = "output"')
    IO.add_argument("--resolution", default="1.0", help="Resolution for leiden clustering.")
    IO.add_argument("--n_neighbors", default="10", help="Number of neighbors for KNN classification.")
    return parser.parse_args()

def leiden_clustering(df, n_neighbors_list, leiden_resolution_list, genome_IDs, cluster_assignments, labels, outpref, write):
    best_params = {"K": None, "resolution": None, "ARI": 0, "AMI": 0}

    per_iteration_accuracy = []
    for n_neighbors in n_neighbors_list:
        print(f"K: {n_neighbors}")

        # Convert distance -> similarity
        S = df.copy()
        S.data = 1.0 - S.data
        N = S.shape[0]

        rows, cols, vals = [], [], []

        for i in range(N):
            start, end = S.indptr[i], S.indptr[i + 1]
            row_indices = S.indices[start:end]
            row_data = S.data[start:end]

            if row_data.size == 0:
                continue

            if row_data.size > n_neighbors:
                topk = np.argpartition(-row_data, n_neighbors)[:n_neighbors]
            else:
                topk = np.arange(row_data.size)

            rows.extend([i] * len(topk))
            cols.extend(row_indices[topk])
            vals.extend(row_data[topk])

        connectivities = csr_matrix((vals, (rows, cols)), shape=S.shape)

        # Symmetrize
        connectivities = 0.5 * (connectivities + connectivities.T)

        for leiden_resolution in leiden_resolution_list:
            print(f"Leiden: {leiden_resolution}")

            adata = sc.AnnData(np.zeros((connectivities.shape[0], 1)))
            adata.obsp["connectivities"] = connectivities

            # debugging
            #G = adata.obsp["connectivities"]
            #print("nnz:", G.nnz)
            #print("min:", G.data.min() if G.nnz > 0 else None)
            #print("max:", G.data.max() if G.nnz > 0 else None)

            sc.tl.leiden(
                adata,
                resolution=leiden_resolution,
                adjacency=adata.obsp["connectivities"],
                key_added="leiden",
                flavor="igraph",
                n_iterations=2
            )
            
            adata.uns["neighbors"] = {
                "connectivities_key": "connectivities",
                "distances_key": None,
                "params": {
                    "method": "custom",
                    "metric": "precomputed",
                },
            }

            leiden_labels = adata.obs["leiden"].astype(int).to_numpy()

            if labels != None:

                true_labels = np.array(cluster_assignments)

                if write:
                    df_true = pd.DataFrame(columns=['Taxon', 'predicted_label', 'true_label'])
                    df_true['Taxon'] = genome_IDs
                    df_true['predicted_label'] = leiden_labels
                    df_true['true_label'] = true_labels

                    df_true.to_csv(outpref + f"_K_{n_neighbors}_resolution_{leiden_resolution}_true.tsv", sep='\t', index=False)

                    df_pred = pd.DataFrame(columns=['Taxon', 'predicted_label'])
                    df_pred['Taxon'] = genome_IDs
                    df_pred['predicted_label'] = leiden_labels
                    df_pred.to_csv(outpref + f"_K_{n_neighbors}_resolution_{leiden_resolution}_predictions.tsv", sep='\t', index=False)

                    # write UMAP
                    sc.tl.umap(adata, neighbors_key=None)
                    df_umap = pd.DataFrame(
                        adata.obsm["X_umap"],
                        columns=["UMAP1", "UMAP2"],
                        index=adata.obs_names
                    )
                    df_umap['Taxon'] = genome_IDs
                    df_umap["leiden"] = adata.obs["leiden"].astype(str)
                    df_umap.to_csv(outpref + f"_K_{n_neighbors}_resolution_{leiden_resolution}_UMAP.csv", sep=',', index=False)

                ari = adjusted_rand_score(true_labels, leiden_labels)
                ami = adjusted_mutual_info_score(true_labels, leiden_labels)
                
                per_iteration_accuracy.append({
                    'K': n_neighbors,
                    'Leiden_resolution': leiden_resolution,
                    'ARI': ari,
                    'AMI': ami
                })

                best_AMI = best_params["AMI"]

                if ami > best_AMI:
                    best_params = {"K": n_neighbors, "resolution": leiden_resolution, "ARI": ari, "AMI": ami}
            else:
                df_pred = pd.DataFrame(columns=['Taxon', 'predicted_label'])
                df_pred['Taxon'] = genome_IDs
                df_pred['predicted_label'] = leiden_labels
                df_pred.to_csv(outpref + f"_K_{n_neighbors}_resolution_{leiden_resolution}_predictions.tsv", sep='\t', index=False)

                sc.tl.umap(adata, neighbors_key=None)
                df_umap = pd.DataFrame(
                    adata.obsm["X_umap"],
                    columns=["UMAP1", "UMAP2"],
                    index=adata.obs_names
                )
                df_umap['Taxon'] = genome_IDs
                df_umap["leiden"] = adata.obs["leiden"].astype(str)
                df_umap.to_csv(outpref + f"_K_{n_neighbors}_resolution_{leiden_resolution}_UMAP.csv", sep=',', index=False)
    
    return best_params, per_iteration_accuracy

def main():
    options = get_options()
    distances = options.distances
    samples = options.samples
    labels = options.labels
    outpref = options.outpref

    # parse genome ids and remove file extensions
    with open(samples, 'rb') as f:
        genome_IDs = pickle.load(f)
    genome_IDs = genome_IDs[0]
    genome_IDs = [os.path.splitext(os.path.splitext(x)[0])[0] for x in genome_IDs]

    labels_dict = OrderedDict()
    original_labels_dict =  OrderedDict()
    cluster_assignments = []
    if labels != None:
        print("Reading labels...")
        with open(labels, "r") as i1:
            i1.readline()
            for line in i1:
                split_line = line.rstrip().split(",")
                labels_dict[split_line[0]] = split_line[1]
                original_labels_dict[split_line[0]] = split_line[1]

        # get metadata, ensuring in same order as files passed
        sample_list = [x for x in genome_IDs if x in labels_dict]
        cluster_list = [str(labels_dict[x]) for x in genome_IDs if x in labels_dict]

        # get all original clusters
        cluster_assignments = [original_labels_dict[x] for x in genome_IDs if x in original_labels_dict]

    print("Reading distances...")
    df = sp.sparse.load_npz(distances).tocsr()

    # Replace infinities with a large finite value (1.0, maximum distance)
    finite_mask = np.isfinite(df.data)
    if not finite_mask.any():
        raise ValueError("All stored entries are infinite.")
    fill_value = 1.0
    df.data = np.where(np.isfinite(df.data), df.data, fill_value).astype(np.float32)

    # Ensure symmetry and zero diagonal
    df = df.maximum(df.T)
    df.setdiag(0)

    # Identify isolated rows (no neighbors)
    row_counts = np.diff(df.indptr)
    isolated = np.where(row_counts == 0)[0]
    print(f"Found {len(isolated)} isolated rows")

    if len(isolated) == df.shape[0]:
        raise ValueError("All rows are isolated — cannot run UMAP.")

    # Remove isolated rows, default is 15 nearest neighbours
    threshold = 1
    mask_keep = row_counts >= threshold
    kept_idx = np.where(mask_keep)[0]
    removed_idx = np.where(~mask_keep)[0]
    print(f"Removing {len(removed_idx)} rows: {removed_idx}")

    # remove rows that are not part of the embedding
    df2 = df[mask_keep][:, mask_keep].tocsr()
    genome_IDs_kept = [genome_IDs[i] for i in kept_idx]
    genome_IDs = genome_IDs_kept
    assert df2.shape[0] == len(genome_IDs_kept)  # should pass now

    leiden_resolution_list = [float(j) for j in options.resolution.split(",")]
    n_neighbors_list = [int(k) for k in options.n_neighbors.split(",")]

    print("Iterating through nearest neighbours...")
    best_params, per_iteration_accuracy = leiden_clustering(df2, n_neighbors_list, leiden_resolution_list, genome_IDs, cluster_assignments, labels, outpref, write=False)

    if labels != None:
        per_label_df = pd.DataFrame(per_iteration_accuracy)

        # Save to TSV
        per_label_df.to_csv(outpref + f"_per_iter_accuracy.tsv", sep='\t', index=False)

        print(best_params)
        _ = leiden_clustering(df2, [best_params["K"]], [best_params["resolution"]], genome_IDs, cluster_assignments, labels, outpref, write=True)

if __name__ == "__main__":
    main()