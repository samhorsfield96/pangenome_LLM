
import argparse
import umap
import umap.plot
import pandas as pd
import pickle
from collections import OrderedDict, Counter
from itertools import chain
import numpy as np
import scipy as sp
from scipy.sparse import coo_matrix
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_samples
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import os

def remove_coo(M: coo_matrix, to_remove):
    to_remove = np.array(to_remove)

    # Rows/cols to keep
    rows_to_keep = np.setdiff1d(np.arange(M.shape[0]), to_remove)
    cols_to_keep = np.setdiff1d(np.arange(M.shape[1]), to_remove)

    # Mapping old -> new indices
    row_map = {old: new for new, old in enumerate(rows_to_keep)}
    col_map = {old: new for new, old in enumerate(cols_to_keep)}

    # Mask out entries in removed rows/cols
    mask = (~np.isin(M.row, to_remove)) & (~np.isin(M.col, to_remove))

    # Reindex surviving entries
    new_rows = np.array([row_map[r] for r in M.row[mask]])
    new_cols = np.array([col_map[c] for c in M.col[mask]])

    # Build reduced COO
    return coo_matrix(
        (M.data[mask], (new_rows, new_cols)),
        shape=(len(rows_to_keep), len(cols_to_keep)),
        dtype=M.dtype
    )

def get_options():
    description = "Plots UMAP of ppsketchlib distances"
    parser = argparse.ArgumentParser(description=description,
                                        prog='python plot_embeddings_sparse.py')
    IO = parser.add_argument_group('Input/options.out')
    IO.add_argument('--distances',
                    required=True,
                    help='Distances .npz file generated by ppsketchlib')
    IO.add_argument('--samples',
                    required=True,
                    help='Samples .pkl file generated by ppsketchlib')
    IO.add_argument('--labels',
                    default=None,
                    help='PopPUNK csv file describing genome names in first column in same order as in embeddings file. Can have second column with assigned clusters.')
    IO.add_argument('--outpref',
                default="output",
                help='Output prefix. Default = "output"')
    return parser.parse_args()

def main():
    options = get_options()
    distances = options.distances
    samples = options.samples
    labels = options.labels
    outpref = options.outpref

    cluster_assignments = []
    if labels != None:
        print("Reading labels...")
        with open(labels, "r") as i1:
            i1.readline()
            for line in i1:
                split_line = line.rstrip().split(",")
                cluster_assignments.append(split_line[1])
        
    # parse genome ids and remove file extensions
    with open(samples, 'rb') as f:
        genome_IDs = pickle.load(f)
    genome_IDs = genome_IDs[0]
    genome_IDs = [os.path.splitext(os.path.splitext(x)[0])[0] for x in genome_IDs]

    print("Reading distances...")
    df = sp.sparse.load_npz(distances).tocsr()

    # Replace infinities with a large finite value (1.0, maximum distance)
    finite_mask = np.isfinite(df.data)
    if not finite_mask.any():
        raise ValueError("All stored entries are infinite.")
    fill_value = 1.0
    df.data = np.where(np.isfinite(df.data), df.data, fill_value).astype(np.float32)

    # Ensure symmetry and zero diagonal
    df = df.maximum(df.T)
    df.setdiag(0)

    # Identify isolated rows (no neighbors)
    row_counts = np.diff(df.indptr)
    isolated = np.where(row_counts == 0)[0]
    print(f"Found {len(isolated)} isolated rows")

    if len(isolated) == df.shape[0]:
        raise ValueError("All rows are isolated â€” cannot run UMAP.")

    # Remove isolated rows, default is 15 nearest neighbours
    threshold = 15
    mask_keep = row_counts >= threshold
    kept_idx = np.where(mask_keep)[0]
    removed_idx = np.where(~mask_keep)[0]
    print(f"Removing {len(removed_idx)} rows: {removed_idx}")

    # remove rows that are not part of the embedding
    df2 = df[mask_keep][:, mask_keep].tocsr()
    genome_IDs_kept = [genome_IDs[i] for i in kept_idx]
    genome_IDs = genome_IDs_kept
    assert df2.shape[0] == len(genome_IDs_kept)  # should pass now

    per_iteration_accuracy = []

    print("Iterating through nearest neighbours...")
    for n_neighbors in n_neighbors_list:
        print(f"K: {n_neighbors}")
        for leiden_resolution in leiden_resolution_list:
            print(f"Leiden: {leiden_resolution}")

            adata = sc.AnnData(np.zeros((df2.shape[0], 1)))
            adata.obsp["connectivities"] = connectivities

            sc.tl.leiden(
                adata,
                resolution=leiden_resolution,
                key_added="leiden",
                flavor="igraph",
                n_iterations=2
            )

            leiden_labels = adata.obs["leiden"].astype(int).to_numpy()

            df_pred = pd.DataFrame(columns=['Taxon', 'predicted_label'])
            df_pred['Taxon'] = prompt_list_df.iloc[:, 0].values
            df_pred['predicted_label'] = leiden_labels
            df_pred.to_csv(args.outpref + f"_K_{n_neighbors}_resolution_{leiden_resolution}_predictions.tsv", sep='\t', index=False)

            if labels != None:

                true_labels = np.array(cluster_assignments)

                df_true = pd.DataFrame(columns=['Taxon', 'predicted_label', 'true_label'])
                df_true['Taxon'] = prompt_list_df.iloc[:, 0].values
                df_true['predicted_label'] = leiden_labels
                df_true['true_label'] = true_labels

                df_true.to_csv(args.outpref + f"_K_{n_neighbors}_resolution_{leiden_resolution}_true.tsv", sep='\t', index=False)

                ari = adjusted_rand_score(true_labels, leiden_labels)
                ami = adjusted_mutual_info_score(true_labels, leiden_labels)
                
                per_iteration_accuracy.append({
                    'K': n_neighbors,
                    'Leiden_resolution': leiden_resolution,
                    'ARI': ari,
                    'AMI': ami
                })
    

    if args.prompt_labels != None:
        per_label_df = pd.DataFrame(per_iteration_accuracy)

        # Save to TSV
        per_label_df.to_csv(args.outpref + f"_per_iter_accuracy.tsv", sep='\t', index=False)



if __name__ == "__main__":
    main()