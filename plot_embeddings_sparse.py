
import argparse
import umap
import umap.plot
import pandas as pd
import pickle
from collections import OrderedDict, Counter
from itertools import chain
import numpy as np
import scipy as sp
from scipy.sparse import coo_matrix
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_samples
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import os

def remove_coo(M: coo_matrix, to_remove):
    to_remove = np.array(to_remove)

    # Rows/cols to keep
    rows_to_keep = np.setdiff1d(np.arange(M.shape[0]), to_remove)
    cols_to_keep = np.setdiff1d(np.arange(M.shape[1]), to_remove)

    # Mapping old -> new indices
    row_map = {old: new for new, old in enumerate(rows_to_keep)}
    col_map = {old: new for new, old in enumerate(cols_to_keep)}

    # Mask out entries in removed rows/cols
    mask = (~np.isin(M.row, to_remove)) & (~np.isin(M.col, to_remove))

    # Reindex surviving entries
    new_rows = np.array([row_map[r] for r in M.row[mask]])
    new_cols = np.array([col_map[c] for c in M.col[mask]])

    # Build reduced COO
    return coo_matrix(
        (M.data[mask], (new_rows, new_cols)),
        shape=(len(rows_to_keep), len(cols_to_keep)),
        dtype=M.dtype
    )

def get_options():
    description = "Plots UMAP of ppsketchlib distances"
    parser = argparse.ArgumentParser(description=description,
                                        prog='python plot_embeddings_sparse.py')
    IO = parser.add_argument_group('Input/options.out')
    IO.add_argument('--distances',
                    required=True,
                    help='Distances .npz file generated by ppsketchlib')
    IO.add_argument('--samples',
                    required=True,
                    help='Samples .pkl file generated by ppsketchlib')
    IO.add_argument('--labels',
                    default=None,
                    help='PopPUNK csv file describing genome names in first column in same order as in embeddings file. Can have second column with assigned clusters.')
    IO.add_argument('--max-labels',
                    default=None,
                    type=int,
                    help='Maximum number of colours to plot. Will plot largest N labels and set all rest as the same')
    IO.add_argument('--outpref',
                default="output",
                help='Output prefix. Default = "output"')
    return parser.parse_args()

def main():
    options = get_options()
    distances = options.distances
    samples = options.samples
    labels = options.labels
    outpref = options.outpref

    labels_dict = OrderedDict()
    original_labels_dict =  OrderedDict()
    if labels != None:
        print("Reading labels...")
        with open(labels, "r") as i1:
            i1.readline()
            for line in i1:
                split_line = line.rstrip().split(",")
                labels_dict[split_line[0]] = split_line[1]
                original_labels_dict[split_line[0]] = int(split_line[1])

        if options.max_labels != None:
            counter = Counter([x for x in labels_dict.values()])
            #print(counter)

            # Get the top entries
            top_labels = set([item for item, _ in counter.most_common(options.max_labels)])

            filtered_data = OrderedDict()
            for key, val in labels_dict.items():
                filtered_data[key] = "0" if val not in top_labels else int(val)

            labels_dict = filtered_data
            #print(labels_dict)
            #print(top_labels)
        
    # read embeddings
    print("Reading distances...")
    df = sp.sparse.load_npz(distances).tocsr()
    #print(df)

    # parse genome ids and remove file extensions
    with open(samples, 'rb') as f:
        genome_IDs = pickle.load(f)
    genome_IDs = genome_IDs[0]
    genome_IDs = [os.path.splitext(os.path.splitext(x)[0])[0] for x in genome_IDs]

    # remove infintities
    df.data = np.nan_to_num(df.data, posinf=0, neginf=0).astype(np.float32)
   
    data = df.data
    print("NaNs:", np.isnan(data).sum())
    print("Infs:", np.isinf(data).sum())
    print("Max:", data.max(), "Min:", data.min())

    df = df.maximum(df.T)   # ensures symmetry, keeps sparsity
    df.setdiag(0)

    reducer = umap.UMAP(metric='precomputed', random_state=42)

    print("Generating UMAP...")
    mapper = reducer.fit(df)
    UMAP_embedding = reducer.transform(df)

    p = umap.plot.connectivity(mapper, show_points=True)
    plt.savefig(outpref + "_connectivity.png", dpi=300, bbox_inches="tight")
    plt.close()

    # if no labels present, all points same colour
    if labels == None:
        sample_list = [x for x in genome_IDs]
        cluster_list = ["0" for x in genome_IDs]
        
    else:
        # get metadata, ensuring in same order as files passed
        sample_list = [x for x in genome_IDs if x in labels_dict]
        cluster_list = [str(labels_dict[x]) for x in genome_IDs if x in labels_dict]

        # get all original clusters
        original_cluster_list = [original_labels_dict[x] for x in genome_IDs if x in original_labels_dict]

        # calculate silhouette score per label for all values
        silhouette_vals = silhouette_samples(df, original_cluster_list)

        # convert to numpy arrays for easy indexing
        cluster_array = np.array(original_cluster_list)
        silhouette_vals = np.array(silhouette_vals)

        unique_labels = np.unique(cluster_array)

        silhouette_per_label = []
        for label in unique_labels:
            idx = cluster_array == label
            label_count = np.sum(idx)
            # Select silhouette scores where cluster is `label`
            silhouette_per_label.append({
                'Label': label,
                'Silhouette_width': silhouette_vals[idx].mean(),
                'Count': label_count
            })

        silhouette_mean = silhouette_vals.mean()
        silhouette_per_label.append({
            'Label': "overall",
            'Silhouette_width': silhouette_mean,
            'Count' : len(original_cluster_list)
        })
        
        silhouette_per_label_df = pd.DataFrame(silhouette_per_label)

        # Save to TSV
        silhouette_per_label_df.to_csv(outpref + "_silhouette.tsv", sep='\t', index=False)

    UMAP_embedding_df = pd.DataFrame(UMAP_embedding)
    UMAP_embedding_df.insert(loc=0, column='Cluster', value=cluster_list)
    UMAP_embedding_df.insert(loc=0, column='Sample', value=sample_list)

    UMAP_embedding_df.columns = ['Sample', 'Cluster', 'UMAP1', 'UMAP2']
    UMAP_embedding_df.to_csv(outpref + '_UMAP.csv', index=False)

    print("Plotting UMAP...")
    
    # set colour scheme
    unique_labels = list(set(cluster_list))
    cmap = plt.colormaps["rainbow"]
    # Assign colors: 0 -> grey, others follow the theme sequence
    color_key = {}
    color_key["0"] = "#808080"

    # Assign colors for all labels except 0
    n_colours = len(unique_labels) - 1
    for idx, lbl in enumerate([l for l in unique_labels if l != "0"]):
        frac = idx / max(1, n_colours - 1)
        rgba = cmap(frac)          # (r,g,b,a)
        hexcol = mcolors.to_hex(rgba[:3])   # drop alpha -> hex like '#aabbcc'
        color_key[str(lbl)] = hexcol   

    print(f"Colour key:\n{color_key}")

    p = umap.plot.points(mapper, labels=UMAP_embedding_df['Cluster'], color_key=color_key, background="black")        

    print("Saving file...")
    plt.savefig(outpref + "_UMAP.png", dpi=300, bbox_inches="tight")
    plt.close()

if __name__ == "__main__":
    main()