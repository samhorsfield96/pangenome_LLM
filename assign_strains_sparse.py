import argparse
import pandas as pd
import pickle
import numpy as np
import scipy as sp
import os
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score
from collections import Counter

def knn_predict(dist_test, y_train, k=3):
    """
    Custom k-NN prediction using a precomputed distance array.

    Parameters
    ----------
    dist_test : np.ndarray, shape (n_test, n_train)
        Precomputed distances from each test sample to each training sample.
    y_train : np.ndarray, shape (n_train,)
        Labels of the training samples.
    k : int
        Number of nearest neighbors to consider.

    Returns
    -------
    y_pred : np.ndarray, shape (n_test,)
        Predicted labels for the test samples.
    """
    n_test, n_train = dist_test.shape
    y_pred = np.empty(n_test, dtype=y_train.dtype)

    for i in range(n_test):
        # Get indices of k smallest distances for test sample i
        nearest_idx = np.argpartition(dist_test[i], k)[:k]  # fast partial sort
        nearest_labels = y_train[nearest_idx]

        # Majority vote
        counts = Counter(nearest_labels)
        y_pred[i] = counts.most_common(1)[0][0]

    return y_pred

def long_to_square(dist_vec, n, m):
    """
    Convert flat distance vector into an n√óm NumPy array (row-major).
    """
    if dist_vec.size != n * m:
        raise ValueError(f"dist_vec length {dist_vec.size} != n*m ({n*m})")
    return dist_vec.reshape((n, m), order="C")  # row-major

def read_distances_file(distances, samples, genome_labels):
    # parse genome ids and remove file extensions
    with open(samples, 'rb') as f:
        genome_IDs = pickle.load(f)

    test_genome_IDs = genome_IDs[1]
    train_genome_IDs = genome_IDs[0]
    test_genome_IDs = [os.path.splitext(os.path.splitext(x)[0])[0] for x in test_genome_IDs]
    train_genome_IDs = [os.path.splitext(os.path.splitext(x)[0])[0] for x in train_genome_IDs]

    # read embeddings
    if ".npz" in distances:
        distance_matrix = sp.sparse.load_npz(distances).tocsr()        
    else:
        distance_matrix = np.load(distances)
        # sample just accessory distances
        distance_matrix = long_to_square(distance_matrix[:, [1]], len(test_genome_IDs), len(train_genome_IDs))

    # reorder
    label_to_idx = {label: i for i, label in enumerate(genome_labels)}
    idx = [label_to_idx[x] for x in test_genome_IDs if x in label_to_idx]
    dist = distance_matrix[idx, :]

    return dist, test_genome_IDs

def get_options():
    description = "Assigns strains using ppsketchlib distances"
    parser = argparse.ArgumentParser(description=description,
                                        prog='python assign_strains_sparse.py')
    IO = parser.add_argument_group('Input/options.out')
    IO.add_argument('--query_distances',
                    required=True,
                    help='Distances .npz/.npy file generated by ppsketchlib for kNN querying')
    IO.add_argument('--query_samples',
                    required=True,
                    help='Samples .pkl file generated by ppsketchlib for kNN querying')
    IO.add_argument('--train_labels',
                    required=True,
                    help='PopPUNK csv file describing genome names in first column in same order as in embeddings file for kNN training.')
    IO.add_argument('--query_labels',
                    required=True,
                    help='PopPUNK csv file describing genome names in first column in same order as in embeddings file for kNN querying.')
    IO.add_argument("--n_neighbors", default="5", help="Number of neighbors for KNN classification.")
    IO.add_argument('--outpref',
                default="output",
                help='Output prefix. Default = "output"')
    
    return parser.parse_args()

def main():
    options = get_options()
    query_distances = options.query_distances
    query_samples = options.query_samples
    train_labels = options.train_labels
    query_labels = options.query_labels
    outpref = options.outpref

    train_genome_labels = []
    train_cluster_assignments = []
    with open(options.train_labels, "r") as i:
        i.readline()
        for line in i:
            split_line = line.rstrip().split(",")
            genome_name = split_line[0]
            train_genome_labels.append(genome_name)
            train_cluster_assignments.append(split_line[1])
    
    query_genome_labels = []
    query_cluster_assignments = []
    with open(options.query_labels, "r") as i:
        i.readline()
        for line in i:
            split_line = line.rstrip().split(",")
            genome_name = split_line[0]
            query_genome_labels.append(genome_name)
            query_cluster_assignments.append(split_line[1])


    print("Reading distances")
    dist_test, test_genome_IDs = read_distances_file(query_distances, query_samples, query_genome_labels)
    
    #print(f"dist_test: {dist_test.shape}")

    y_train = np.array(train_cluster_assignments)
    y_test = np.array(query_cluster_assignments)

    #print(f"y_test: {y_test.shape}")

    print("Running kNN")
    n_neighbors_list = [int(k) for k in options.n_neighbors.split(",")]
    per_k_accuracy = []
    for n_neighbors in n_neighbors_list:
        try:
            # predict from classifier
            y_pred = knn_predict(dist_test, y_train, k=n_neighbors)

            query_list_df_pred = pd.DataFrame(columns=['Taxon', 'predicted_label'])
            query_list_df_pred['Taxon'] = query_genome_labels
            query_list_df_pred['predicted_label'] = y_pred
            query_list_df_pred.to_csv(options.outpref + f"_k_{n_neighbors}_predictions.tsv", sep='\t', index=False)
           
            unique_labels = np.unique(y_test)
            # Per-class accuracy
            per_label_accuracy = []
            for label in unique_labels:
                # Select only test examples of this class
                idx = y_test == label
                y_test_label_count = np.sum(idx)
                y_train_label_count = np.sum(y_train == label)
                label_acc = accuracy_score(y_test[idx], y_pred[idx])
                label_precision = precision_score(y_test, y_pred, labels=[label], average='macro', zero_division=0)
                label_recall = recall_score(y_test, y_pred, labels=[label], average='macro', zero_division=0)

                per_label_accuracy.append({
                    'K': n_neighbors,
                    'Label': label,
                    'Precision': label_precision,
                    'Recall': label_recall,
                    'Accuracy': label_acc,
                    'Query_count' : y_test_label_count,
                    'Test_count': y_train_label_count
                })
            
            # Overall accuracy
            all_acc = accuracy_score(y_test, y_pred)
            all_acc_balanced = balanced_accuracy_score(y_test, y_pred)
            overall_precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
            overall_recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)
            overall_precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            overall_recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)

            per_label_accuracy.append({
                'K': n_neighbors,
                'Label': "overall",
                'Accuracy': all_acc,
                'Precision': overall_precision_macro,
                'Recall': overall_recall_macro,
                'Query_count' : len(query_cluster_assignments),
                'Test_count': len(train_cluster_assignments)
                })
            per_label_accuracy.append({
                'K': n_neighbors,
                'Label': "overall_balanced",
                'Accuracy': all_acc_balanced,
                'Precision': overall_precision_weighted,
                'Recall': overall_recall_weighted,
                'Query_count' : len(query_cluster_assignments),
                'Test_count': len(train_cluster_assignments)
                })
            
            # save to overall dataframe:
            per_k_accuracy.append({
                'K': n_neighbors,
                'Label': "overall",
                'Accuracy': all_acc,
                'Precision': overall_precision_macro,
                'Recall': overall_recall_macro,
                'Query_count' : len(query_cluster_assignments),
                'Test_count': len(train_cluster_assignments)
            })
            per_k_accuracy.append({
                'K': n_neighbors,
                'Label': "overall_balanced",
                'Accuracy': all_acc_balanced,
                'Precision': overall_precision_weighted,
                'Recall': overall_recall_weighted,
                'Query_count' : len(query_cluster_assignments),
                'Test_count': len(train_cluster_assignments)
            })

            # Convert to DataFrame
            per_label_df = pd.DataFrame(per_label_accuracy)

            # Save to TSV
            per_label_df.to_csv(options.outpref + f"_k_{n_neighbors}_per_label_accuracy.tsv", sep='\t', index=False)
        except Exception as error:
            print(f"Failed to train at K={n_neighbors}")
            print("Error here:", error)
            continue
    
    # Convert to DataFrame
    per_k_df = pd.DataFrame(per_k_accuracy)

    # Save to TSV
    per_k_df.to_csv(options.outpref + "_overall_accuracy.tsv", sep='\t', index=False)

if __name__ == "__main__":
    main()